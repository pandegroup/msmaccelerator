#!/usr/bin/env python

'''
This is a daemon process that iteratively ssh connects
to a list of clusters, and attempts to maintain a certain
number of workqueue workers on each cluster.
'''

import sys, os, re
import time
import subprocess
from subprocess import PIPE
import yaml


def load_config( config_file ):

    with open(config_file) as f:
        config = yaml.load(f)

    host       = config['host']
    port       = int(config['port'])
    sleep_time = int(config['sleep_time'])

    server_info = {}
    for entry in config['servers']:
        try:
            server_info[ entry['name'] ] = { 'username'    : entry['username'],
                                             'queue'       : entry['queue'],
                                             'command'     : entry['command'],
                                             'walltime'    : entry['walltime'],
                                             'target_jobs' : entry['target_jobs'],
                                             'ppn'         : entry['ppn'],
                                             'workers_per_node': entry['workers_per_node'] }
        except:
            raise Exception("Error parsing entry:", entry)

    return server_info, host, port, sleep_time


def check(server, username, max_attempts=10):
    """ Checks how many 'MSMa-worker' jobs you currently have running on the cluster """

    cmd = "ssh %s@%s 'qstat -u %s | grep MSMa-worker | wc -l'" % (username, server, username)

    attempted = 0 # sometimes ssh is flaky - we should try many times
    while attempted < max_attempts:
        p = subprocess.Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
        stdout, stderr = p.communicate() 
        if stderr == '':
            try:
                active_jobs = int(stdout.strip())
            except:
                active_jobs = 0
                print "Trouble parsing 'qstat' output, but I think the following means no jobs running:"
                print stdout
            break
        else:
            attempted += 1

    if attempted == max_attempts:
        print "Tried %d times to connect, limit: %d" % (attempted, max_attempts)
        #raise Exception("Could not retrieve job count from server: %s, (%s)" % (server, stderr))

    return active_jobs


def submit(server, server_info, num_jobs, host, port, max_attempts=10):
    """ Submits a command to the remote cluster, telling it to submit a number of workers """

    # Use: pbs_submit_workers [options] <servername> <port> <num-nodes> <ppn> <pbs-queue> <walltime> <workers-per-node>
    work_queue_command = "%s %s %d %d %d %s %s %d" % ( server_info[server]['command'],
                                                       host, port, num_jobs,
                                                       server_info[server]['ppn'],
                                                       server_info[server]['queue'],
                                                       server_info[server]['walltime'],
                                                       server_info[server]['workers_per_node'] )

    cmd = "ssh %s@%s '%s'" % (server_info[server]['username'], server, work_queue_command)

    attempted = 0 # sometimes ssh is flaky - we should try many times
    while attempted < max_attempts:
        p = subprocess.Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
        stdout, stderr = p.communicate()
        if stderr == '':
            break
        else:
            attempted += 1

    if attempted == max_attempts:
        print "Tried %d times to connect, limit: %d" % (attempted, max_attempts)
        #raise Exception("Could not retrieve job count from server: %s, (%s)" % (server, stderr))

    print "\tExecuting: %s" % cmd
    return


def is_msmaccelerator_alive():
    """ Checks if there are any locally running MSMAccelerator masters """

    processname = 'MSMAccelerator'

    is_alive = False # guilty until proven innocent

    for line in os.popen("ps -a"):
        if line.find(processname) > 0:
            is_alive = True

    return is_alive


def main(config_file):

    # parse a yaml input file into the server_info object
    server_info, host, port, sleep_time = load_config( config_file )

    # loop continuously, checking and submiting jobs. Die only when
    # there are no local MSMAccelerator processes
    keep_going = True
    while keep_going:
        print "\nChecking all servers:"

        for server in server_info.keys(): 
            active_jobs = check( server, server_info[server]['username'] )
            jobs_to_submit = server_info[server]['target_jobs'] - active_jobs
            if jobs_to_submit > 0:
                print "Submitting %d jobs to %s..." % (jobs_to_submit, server)
                submit(server, server_info, jobs_to_submit, host, port)
            else:
                print "%d jobs already running on %s. Waiting." % (active_jobs, server)

        keep_going = is_msmaccelerator_alive()
        time.sleep(sleep_time)

    print "No MSMAccelerator master processes found. Exited."

    return


if __name__ == '__main__':

    print sys.argv
    print "Use: %s <servers_list.yaml>\n" % sys.argv[0]
    config_file = sys.argv[1]

    main(config_file)

